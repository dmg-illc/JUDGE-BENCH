{"Mistral-7B-Instruct-v0.3 (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": 0.14816332652794245, "spearman": 0.15060301870416878, "kendall": 0.14270242250649098}, "p_value": {"pearson": 0.00667537065043217, "spearman": 0.00581920930003974, "kendall": 0.0059914818481938215}, "kappa_score": 0.0015681662256199491, "total_responses": 350, "valid_responses": 334, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "Starling-LM-7B-alpha (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.0009448115934738398, "spearman": -0.0018454392241768125, "kendall": -0.001772122022986036}, "p_value": {"pearson": 0.986069750538357, "spearman": 0.9727948209366419, "kendall": 0.9727351995970567}, "kappa_score": -0.06754812551276412, "total_responses": 350, "valid_responses": 344, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "Meta-Llama-3.1-8B-Instruct (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.08915736536183819, "spearman": -0.13702910960560583, "kendall": -0.13306468870829793}, "p_value": {"pearson": 0.09680719368637611, "spearman": 0.010493439650987446, "kendall": 0.010693157789542355}, "kappa_score": -0.18901769371568022, "total_responses": 350, "valid_responses": 348, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "c4ai-command-r-v01 (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.20075514403206965, "spearman": -0.20053998736623618, "kendall": -0.19997937330857934}, "p_value": {"pearson": 0.00015625530985663587, "spearman": 0.00015887223925113272, "kendall": 0.00017939322994648882}, "kappa_score": -0.17590149516270892, "total_responses": 350, "valid_responses": 350, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.020398199649295908, "spearman": -0.024356197164268404, "kendall": -0.02368131271043784}, "p_value": {"pearson": 0.706179236939371, "spearman": 0.6525931286777671, "kendall": 0.6519296690012104}, "kappa_score": -0.021085728106242208, "total_responses": 350, "valid_responses": 344, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": 0.019510333011334046, "spearman": 0.031497086036692205, "kendall": 0.030388886372852794}, "p_value": {"pearson": 0.7555882465528941, "spearman": 0.6152455317367963, "kendall": 0.6142941404878826}, "kappa_score": 0.05855982785562408, "total_responses": 350, "valid_responses": 257, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.2526894943122292, "spearman": -0.27252232312066127, "kendall": -0.2621166809588964}, "p_value": {"pearson": 1.7398175859335133e-06, "spearman": 2.3275177674650946e-07, "kendall": 3.69883417565532e-07}, "kappa_score": -0.210296505304308, "total_responses": 350, "valid_responses": 349, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "gemini-1.5-flash-latest (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.16880061792666187, "spearman": -0.18646122642767868, "kendall": -0.18106777788493256}, "p_value": {"pearson": 0.001527295075129471, "spearman": 0.0004537114594579482, "kendall": 0.0004951191591632034}, "kappa_score": -0.13299531981279267, "total_responses": 350, "valid_responses": 350, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}, "gpt-4o (SP: None, AP: 9)": {"safety": {"corr_coeff": {"pearson": -0.3406565397773226, "spearman": -0.36012931709003043, "kendall": -0.3559855011524483}, "p_value": {"pearson": 5.864354805710254e-11, "spearman": 3.710368201976138e-12, "kendall": 1.7228194627191113e-11}, "kappa_score": -0.26013019795403225, "total_responses": 350, "valid_responses": 350, "krippendorff_alpha": 0.16086021565770392, "type": "categorical", "expert": "false", "task": "Toxicity \\ Safety"}}}