{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.3012823112472585, "spearman": 0.34796739286545686, "kendall": 0.2942334633908415}, "p_value": {"pearson": 0.00017046804426974245, "spearman": 1.197886066945788e-05, "kendall": 2.6225155169219752e-05}, "kappa_score": 0.16474266474266475, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.09281872288985206, "spearman": 0.10438386236314931, "kendall": 0.09643524149568265}, "p_value": {"pearson": 0.25698482066335593, "spearman": 0.2021256255506362, "kendall": 0.20568074249295898}, "kappa_score": 0.015995260663507205, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.02617356844329189, "spearman": -0.02617356844329189, "kendall": -0.026173568443291887}, "p_value": {"pearson": 0.7513553492595214, "spearman": 0.7513553492595222, "kendall": 0.7501700077473575}, "kappa_score": -0.014920531949399907, "total_responses": 151, "valid_responses": 149, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.138076441867896, "spearman": -0.138076441867896, "kendall": -0.13807644186789597}, "p_value": {"pearson": 0.0908901968469075, "spearman": 0.09089019684690765, "kendall": 0.09082073378630184}, "kappa_score": -0.06452383797347339, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.35849199318778474, "spearman": 0.3728485423915395, "kendall": 0.3330432062201386}, "p_value": {"pearson": 8.87000125746847e-06, "spearman": 3.5752758400078146e-06, "kendall": 7.806331308828851e-06}, "kappa_score": 0.13256172839506164, "total_responses": 151, "valid_responses": 146, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.28027254368571475, "spearman": 0.24211687062673345, "kendall": 0.2321725143130722}, "p_value": {"pearson": 0.0005131457819657606, "spearman": 0.0028353061568235586, "kendall": 0.0032513096004927523}, "kappa_score": 0.12078710487753808, "total_responses": 151, "valid_responses": 150, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.0019688053109398057, "spearman": -0.001968805310939817, "kendall": -0.001968805310939817}, "p_value": {"pearson": 0.9813154289902654, "spearman": 0.9813154289902666, "kendall": 0.9812167502693686}, "kappa_score": -0.0018416206261511192, "total_responses": 151, "valid_responses": 144, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.16706696460701742, "spearman": -0.1670669646070174, "kendall": -0.1670669646070174}, "p_value": {"pearson": 0.04312216125474734, "spearman": 0.04312216125474747, "kendall": 0.04352071739343653}, "kappa_score": -0.06742738589211617, "total_responses": 151, "valid_responses": 147, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Starling-LM-7B-alpha (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.32227322550110826, "spearman": 0.27524745002433715, "kendall": 0.23952670365717058}, "p_value": {"pearson": 5.449636053734149e-05, "spearman": 0.0006250476807673666, "kendall": 0.0009735345320339763}, "kappa_score": 0.06135135135135139, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.1445090009836422, "spearman": 0.09566619327708385, "kendall": 0.0886925136541379}, "p_value": {"pearson": 0.07667644589543468, "spearman": 0.24261352128778854, "kendall": 0.23994745022090191}, "kappa_score": 0.0535840395480226, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.259425396774636, "spearman": 0.259425396774636, "kendall": 0.259425396774636}, "p_value": {"pearson": 0.0013469322859877343, "spearman": 0.0013469322859877494, "kendall": 0.0015418430309926995}, "kappa_score": 0.17682926829268297, "total_responses": 151, "valid_responses": 150, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.12826451650370663, "spearman": -0.1282645165037066, "kendall": -0.1282645165037066}, "p_value": {"pearson": 0.11652367677501012, "spearman": 0.11652367677500972, "kendall": 0.11620283815719225}, "kappa_score": -0.06393528183716057, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.25042855218576854, "spearman": 0.19390196355837622, "kendall": 0.1755037231821055}, "p_value": {"pearson": 0.001995786323717866, "spearman": 0.017428148284093045, "kendall": 0.017642927631305733}, "kappa_score": 0.07837393727227271, "total_responses": 151, "valid_responses": 150, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.11423534666297389, "spearman": 0.14767394552997998, "kendall": 0.1403150833703091}, "p_value": {"pearson": 0.1653740360326878, "spearman": 0.07229176718891972, "kendall": 0.07282984902690724}, "kappa_score": 0.03844569545516974, "total_responses": 151, "valid_responses": 149, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.05906504542210935, "spearman": 0.05906504542210935, "kendall": 0.05906504542210935}, "p_value": {"pearson": 0.472774717548836, "spearman": 0.4727747175488364, "kendall": 0.4709213512430428}, "kappa_score": 0.055273547659334454, "total_responses": 151, "valid_responses": 150, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.19530883212518396, "spearman": -0.195308832125184, "kendall": -0.19530883212518396}, "p_value": {"pearson": 0.016986258494573452, "spearman": 0.01698625849457331, "kendall": 0.017499826899147027}, "kappa_score": -0.06731682057478205, "total_responses": 151, "valid_responses": 149, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-v01 (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.38210832116967025, "spearman": 0.32249220635666664, "kendall": 0.2992401700137051}, "p_value": {"pearson": 1.2892284326798624e-06, "spearman": 5.382751874013829e-05, "kendall": 7.82775491104838e-05}, "kappa_score": 0.06333282279062658, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.37210938496254853, "spearman": 0.33386550835177986, "kendall": 0.3250367400384181}, "p_value": {"pearson": 2.5433267114595695e-06, "spearman": 2.7973420217703717e-05, "kendall": 4.860235312902498e-05}, "kappa_score": 0.24025157232704408, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.015909315027616774, "spearman": -0.01590931502761675, "kendall": -0.01590931502761675}, "p_value": {"pearson": 0.8483253297269439, "spearman": 0.8483253297269449, "kendall": 0.8475597094969135}, "kappa_score": -0.014559894109861027, "total_responses": 151, "valid_responses": 147, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.13155199123097866, "spearman": -0.13155199123097866, "kendall": -0.13155199123097866}, "p_value": {"pearson": 0.11098895612480326, "spearman": 0.11098895612480383, "kendall": 0.1107160086434067}, "kappa_score": -0.06531591575579831, "total_responses": 151, "valid_responses": 148, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-plus (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.4894552835719074, "spearman": 0.42000864920159653, "kendall": 0.385743415094025}, "p_value": {"pearson": 4.803583229418113e-10, "spearman": 1.6005134015948897e-07, "kendall": 4.553212929631355e-07}, "kappa_score": 0.114899196852975, "total_responses": 151, "valid_responses": 144, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.2876020104565409, "spearman": 0.16358859013609445, "kendall": 0.15450211685789292}, "p_value": {"pearson": 0.0006253203788111878, "spearman": 0.0552141997077647, "kendall": 0.05609517810445328}, "kappa_score": 0.034804329725228955, "total_responses": 151, "valid_responses": 138, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.039827930167112, "spearman": 0.03982793016711201, "kendall": 0.039827930167112016}, "p_value": {"pearson": 0.6272936488492237, "spearman": 0.627293648849224, "kendall": 0.6256982183546937}, "kappa_score": 0.039643841424634285, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.1652746829035575, "spearman": -0.1652746829035575, "kendall": -0.16527468290355754}, "p_value": {"pearson": 0.042558049692702944, "spearman": 0.04255804969270289, "kendall": 0.04295027671323219}, "kappa_score": -0.06567339737306388, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.715442338683465, "spearman": 0.6640328696142077, "kendall": 0.60648100743388}, "p_value": {"pearson": 5.676092966300934e-25, "spearman": 1.4946069872969487e-20, "kendall": 6.350513326791756e-18}, "kappa_score": 0.321566110397946, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.1679353552626328, "spearman": 0.12204983737953552, "kendall": 0.1159598488188924}, "p_value": {"pearson": 0.039289924163989666, "spearman": 0.13546397853069822, "kendall": 0.1346837032873547}, "kappa_score": 0.03646308113035546, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.09248181229135066, "spearman": 0.09248181229135066, "kendall": 0.09248181229135066}, "p_value": {"pearson": 0.25872313105580125, "spearman": 0.2587231310558018, "kendall": 0.25735443458773477}, "kappa_score": 0.03477371516236272, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.2588223626974817, "spearman": 0.2588223626974817, "kendall": 0.2588223626974817}, "p_value": {"pearson": 0.0015487928923204617, "spearman": 0.0015487928923204684, "kendall": 0.0017638229818219338}, "kappa_score": 0.16095890410958902, "total_responses": 151, "valid_responses": 147, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": -0.10475730809378482, "spearman": -0.1027892453597301, "kendall": -0.09634900261212082}, "p_value": {"pearson": 0.34593302286967326, "spearman": 0.3551193503516943, "kendall": 0.3349441727056722}, "kappa_score": -0.012606750711671388, "total_responses": 151, "valid_responses": 83, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 6.938893903907228e-18, "spearman": -0.0019807113105283305, "kendall": -0.0018577022187215323}, "p_value": {"pearson": 1.0000000000000004, "spearman": 0.9862684846543699, "kendall": 0.9861335510152573}, "kappa_score": 0.014775977121067818, "total_responses": 151, "valid_responses": 78, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.010492987563163362, "spearman": 0.010492987563163364, "kendall": 0.010492987563163362}, "p_value": {"pearson": 0.8982502420442876, "spearman": 0.898250242044288, "kendall": 0.8977435435501382}, "kappa_score": 0.010285368559039942, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.15866735070071972, "spearman": -0.15866735070071972, "kendall": -0.15866735070071972}, "p_value": {"pearson": 0.062096748544089905, "spearman": 0.06209674854408977, "kendall": 0.062333033005485664}, "kappa_score": -0.07071599949488583, "total_responses": 151, "valid_responses": 139, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gemini-1.5-flash-latest (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.5282200758582313, "spearman": 0.5393228418759651, "kendall": 0.46840746249934534}, "p_value": {"pearson": 3.1630088390954985e-12, "spearman": 9.018471399038858e-13, "kendall": 2.2464544008436898e-11}, "kappa_score": 0.11308166058394153, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.13675863861994594, "spearman": 0.0878828046449204, "kendall": 0.08120638615077061}, "p_value": {"pearson": 0.0940460988346379, "spearman": 0.28325960558656366, "kendall": 0.2844082492545983}, "kappa_score": 0.006220699603317725, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.008298256439121486, "spearman": -0.008298256439121491, "kendall": -0.008298256439121491}, "p_value": {"pearson": 0.9194511854047847, "spearman": 0.9194511854047835, "kendall": 0.9190484054300995}, "kappa_score": -0.007574391343552644, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.04207189712430772, "spearman": -0.04207189712430773, "kendall": -0.042071897124307735}, "p_value": {"pearson": 0.6080078197168864, "spearman": 0.6080078197168859, "kendall": 0.6063619824227549}, "kappa_score": -0.02182019058141127, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-4o (SP: None, AP: 9)": {"Overall Quality": {"corr_coeff": {"pearson": 0.7264854063534665, "spearman": 0.6972494457496738, "kendall": 0.6249641920056448}, "p_value": {"pearson": 4.7264922166052796e-26, "spearman": 2.6614738946277584e-23, "kendall": 1.2252375117054987e-18}, "kappa_score": 0.30942114203580295, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.42224163696004974, "spearman": 0.2829592741021603, "kendall": 0.2688122769862386}, "p_value": {"pearson": 6.657146008257306e-08, "spearman": 0.0004309059316528815, "kendall": 0.0005499810040856703}, "kappa_score": 0.09907363793374158, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.22310069551512055, "spearman": 0.2231006955151206, "kendall": 0.22310069551512055}, "p_value": {"pearson": 0.005895824008058728, "spearman": 0.005895824008058743, "kendall": 0.006287202065531328}, "kappa_score": 0.16510520657349093, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.1743382792631566, "spearman": -0.1743382792631565, "kendall": -0.1743382792631565}, "p_value": {"pearson": 0.03228001583563597, "spearman": 0.032280015835636125, "kendall": 0.03274474761449562}, "kappa_score": -0.0659483559353522, "total_responses": 151, "valid_responses": 151, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}}