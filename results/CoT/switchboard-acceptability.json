{"Mistral-7B-Instruct-v0.3 (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.4257869879298467, "spearman": 0.35860555786319076, "kendall": 0.3004953755922793}, "p_value": {"pearson": 3.1841014663685375e-05, "spearman": 0.0005589729216096592, "kendall": 0.0006762350952740394}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 89, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-v01 (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.40538474341003505, "spearman": 0.37936603607264474, "kendall": 0.2996595802420262}, "p_value": {"pearson": 2.8644623488185064e-05, "spearman": 9.920838975296486e-05, "kendall": 0.00013072343849063282}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Starling-LM-7B-alpha (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.3460790133402778, "spearman": 0.35452372489773737, "kendall": 0.287871429454928}, "p_value": {"pearson": 0.0004501470707779078, "spearman": 0.00031779104022769156, "kendall": 0.00041145499287536673}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 99, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Meta-Llama-3.1-8B-Instruct (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.19471325351863797, "spearman": 0.17012196092817428, "kendall": 0.12765353820423256}, "p_value": {"pearson": 0.060026577066545427, "spearman": 0.10115120510894428, "kendall": 0.10958924235370579}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 94, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-plus (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.39224208404077243, "spearman": 0.366915522824478, "kendall": 0.30234125518379884}, "p_value": {"pearson": 5.935950853942096e-05, "spearman": 0.0001872945670415885, "kendall": 0.0002677610485535191}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 99, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.14287524380233735, "spearman": 0.0662780068513672, "kendall": 0.05481224562950351}, "p_value": {"pearson": 0.20322024594882415, "spearman": 0.5566143783338369, "kendall": 0.5327720752371436}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 81, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.4858415443316886, "spearman": 0.4757899866996532, "kendall": 0.38080344765127144}, "p_value": {"pearson": 3.4403309956218573e-07, "spearman": 6.453884349086481e-07, "kendall": 1.3495345026504264e-06}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 99, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.13398100178785258, "spearman": 0.08112117287826535, "kendall": 0.06057748851564841}, "p_value": {"pearson": 0.2215281434258782, "spearman": 0.46048760393516464, "kendall": 0.47464104093937254}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 85, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gemini-1.5-flash-latest (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.5445021191354549, "spearman": 0.5277256614238256, "kendall": 0.41073045724903356}, "p_value": {"pearson": 9.772097499840933e-09, "spearman": 3.296456570577905e-08, "kendall": 2.344740783632552e-07}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 96, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gpt-4o (SP: None, AP: 9)": {"acceptability": {"corr_coeff": {"pearson": 0.6558716719832616, "spearman": 0.5953651252725811, "kendall": 0.4885510555797396}, "p_value": {"pearson": 1.3014592016704692e-13, "spearman": 6.443144404693822e-11, "kendall": 9.51376783206851e-10}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}}