{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.39226648832120603, "spearman": 0.39226648832120603, "kendall": 0.39226648832120603}, "p_value": {"pearson": 4.8339445382806414e-39, "spearman": 4.833944538280062e-39, "kendall": 3.850204144743833e-36}, "kappa_score": 0.34840044389243696, "total_responses": 1043, "valid_responses": 1025, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.49906579062603873, "spearman": 0.49906579062603856, "kendall": 0.4990657906260387}, "p_value": {"pearson": 4.7563595198874725e-45, "spearman": 4.756359519887786e-45, "kendall": 1.7629217938882678e-39}, "kappa_score": 0.4705156783469915, "total_responses": 1043, "valid_responses": 695, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.44203265911110645, "spearman": 0.4420326591111068, "kendall": 0.44203265911110673}, "p_value": {"pearson": 5.554066045687417e-51, "spearman": 5.554066045685567e-51, "kendall": 4.595253976864304e-46}, "kappa_score": 0.4145023269779312, "total_responses": 1043, "valid_responses": 1040, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Starling-LM-7B-alpha (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.3914944480069972, "spearman": 0.39149444800699723, "kendall": 0.3914944480069972}, "p_value": {"pearson": 2.5505391050312557e-37, "spearman": 2.550539105031003e-37, "kendall": 1.4493996279566317e-34}, "kappa_score": 0.38909880584543477, "total_responses": 1043, "valid_responses": 982, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "c4ai-command-r-v01 (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.32883913850536045, "spearman": 0.32883913850536056, "kendall": 0.3288391385053605}, "p_value": {"pearson": 7.594162948452503e-27, "spearman": 7.594162948452151e-27, "kendall": 1.7136283167260728e-25}, "kappa_score": 0.29547641963426374, "total_responses": 1043, "valid_responses": 1008, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "c4ai-command-r-plus (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.1701895903059759, "spearman": 0.17018959030597589, "kendall": 0.1701895903059759}, "p_value": {"pearson": 4.927304958665331e-07, "spearman": 4.927304958665189e-07, "kendall": 5.830803601165695e-07}, "kappa_score": 0.05944784356867672, "total_responses": 1043, "valid_responses": 863, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.5193338318273288, "spearman": 0.5193338318273291, "kendall": 0.519333831827329}, "p_value": {"pearson": 1.9155928530523283e-68, "spearman": 1.9155928530520903e-68, "kendall": 4.436409911826091e-59}, "kappa_score": 0.5146325188268565, "total_responses": 1043, "valid_responses": 975, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.5749636641816256, "spearman": 0.5749636641816257, "kendall": 0.5749636641816256}, "p_value": {"pearson": 1.2090078297490551e-92, "spearman": 1.2090078297489805e-92, "kendall": 9.45989816151059e-77}, "kappa_score": 0.5593299428821197, "total_responses": 1043, "valid_responses": 1041, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.31109769041386204, "spearman": 0.3110976904138621, "kendall": 0.31109769041386215}, "p_value": {"pearson": 1.9043918810632547e-07, "spearman": 1.9043918810632502e-07, "kendall": 3.526487654196854e-07}, "kappa_score": 0.2647403358063257, "total_responses": 1043, "valid_responses": 269, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "gemini-1.5-flash-latest (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.45973627253450733, "spearman": 0.45973627253450733, "kendall": 0.45973627253450733}, "p_value": {"pearson": 1.135562627909648e-55, "spearman": 1.1355626279095936e-55, "kendall": 8.039769239586095e-50}, "kappa_score": 0.4502062930272561, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "gpt-4o (SP: None, AP: 9)": {"grammaticality": {"corr_coeff": {"pearson": 0.3959859297913925, "spearman": 0.3959859297913926, "kendall": 0.39598592979139247}, "p_value": {"pearson": 1.7329812379693108e-40, "spearman": 1.7329812379692996e-40, "kendall": 2.0553074048158383e-37}, "kappa_score": 0.3483739486850974, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}}