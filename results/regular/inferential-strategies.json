{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.14920562217376726, "spearman": 0.14920562217376723, "kendall": 0.14920562217376726}, "p_value": {"pearson": 0.009652626682626952, "spearman": 0.009652626682626914, "kendall": 0.009879848607106985}, "kappa_score": 0.1345113595384061, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.40256634686354714, "spearman": 0.4025663468635471, "kendall": 0.40256634686354714}, "p_value": {"pearson": 4.0850026477619397e-13, "spearman": 4.0850026477619226e-13, "kendall": 3.3781091731776544e-12}, "kappa_score": 0.3953136810279667, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.057989862226586336, "spearman": 0.05798986222658634, "kendall": 0.05798986222658635}, "p_value": {"pearson": 0.3167961421669108, "spearman": 0.3167961421669123, "kendall": 0.3159870698145877}, "kappa_score": 0.01452882611715689, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.05393607122564101, "spearman": 0.05393607122564101, "kendall": 0.053936071225640994}, "p_value": {"pearson": 0.3518649380493677, "spearman": 0.3518649380493688, "kendall": 0.35100491413134227}, "kappa_score": 0.03509439707673556, "total_responses": 300, "valid_responses": 158, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.015791098494992103, "spearman": 0.01579109849499207, "kendall": 0.01579109849499207}, "p_value": {"pearson": 0.7853259903322266, "spearman": 0.7853259903322234, "kendall": 0.7848119991989516}, "kappa_score": 0.01297497683039861, "total_responses": 300, "valid_responses": 14, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.12423115207712183, "spearman": -0.1242311520771219, "kendall": -0.12423115207712192}, "p_value": {"pearson": 0.031468029469192985, "spearman": 0.031468029469192714, "kendall": 0.031701250829579246}, "kappa_score": -0.11862036821253774, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.11620026944469597, "spearman": 0.11620026944469582, "kendall": 0.11620026944469579}, "p_value": {"pearson": 0.04431880498852842, "spearman": 0.04431880498852871, "kendall": 0.04450633560583367}, "kappa_score": 0.02664522797748936, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.23004961751511202, "spearman": 0.23004961751511252, "kendall": 0.2300496175151125}, "p_value": {"pearson": 5.770196824360233e-05, "spearman": 5.770196824360139e-05, "kendall": 6.951790127636946e-05}, "kappa_score": 0.22432898300911097, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-4o (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.4224625387437791, "spearman": 0.4224625387437788, "kendall": 0.42246253874377887}, "p_value": {"pearson": 2.0454122610185482e-14, "spearman": 2.0454122610186245e-14, "kendall": 2.7714265080585833e-13}, "kappa_score": 0.41898634453781514, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.09549558753434358, "spearman": 0.09549558753434369, "kendall": 0.09549558753434367}, "p_value": {"pearson": 0.09875859380015985, "spearman": 0.09875859380015886, "kendall": 0.09868282782508439}, "kappa_score": 0.05904059040590415, "total_responses": 300, "valid_responses": 298, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.03036281475502689, "spearman": 0.030362814755026966, "kendall": 0.030362814755026963}, "p_value": {"pearson": 0.6004004642795211, "spearman": 0.6004004642795193, "kendall": 0.5995677947804869}, "kappa_score": 0.017190362124841485, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.01732870321943618, "spearman": -0.017328703219436244, "kendall": -0.017328703219436244}, "p_value": {"pearson": 0.7650076522782606, "spearman": 0.7650076522782602, "kendall": 0.764450785646628}, "kappa_score": -0.016009852216748888, "total_responses": 300, "valid_responses": 292, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}}