{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.5505761567355254, "spearman": 0.5505761567355247, "kendall": 0.5505761567355247}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.5467451947671417, "total_responses": 11288, "valid_responses": 11042, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.24036148640007593, "spearman": 0.24036148640007593, "kendall": 0.24036148640007593}, "p_value": {"pearson": 4.4898869818724356e-148, "spearman": 4.489886981868146e-148, "kendall": 7.840334208076815e-144}, "kappa_score": 0.1371853536346903, "total_responses": 11288, "valid_responses": 11096, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.6004237830991145, "spearman": 0.6004237830991146, "kendall": 0.6004237830991146}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.5888601088668594, "total_responses": 11288, "valid_responses": 11254, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.34631616463322024, "spearman": 0.34631616463322, "kendall": 0.34631616463322}, "p_value": {"pearson": 1.70983891e-315, "spearman": 0.0, "kendall": 2.4129016344465523e-296}, "kappa_score": 0.22623433621722178, "total_responses": 11288, "valid_responses": 11270, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.580155249934572, "spearman": 0.5801552499345717, "kendall": 0.5801552499345717}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.5783926114805955, "total_responses": 11288, "valid_responses": 11251, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.4022787739077318, "spearman": 0.40227877390773176, "kendall": 0.4022787739077317}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.32119723881451623, "total_responses": 11288, "valid_responses": 11284, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.40878033328124697, "spearman": 0.40878033328124724, "kendall": 0.4087803332812473}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.39022075114158594, "total_responses": 11288, "valid_responses": 9702, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.2706991701008289, "spearman": 0.270699170100829, "kendall": 0.27069917010082906}, "p_value": {"pearson": 8.678755694563584e-189, "spearman": 8.678755694561013e-189, "kendall": 6.95813300171652e-182}, "kappa_score": 0.21321131507347713, "total_responses": 11288, "valid_responses": 11019, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.3550559564545347, "spearman": 0.3550559564545124, "kendall": 0.3550559564545123}, "p_value": {"pearson": 1.2456528372218993e-167, "spearman": 1.2456528372856528e-167, "kendall": 5.3239385498506195e-157}, "kappa_score": 0.3443338376037215, "total_responses": 5654, "valid_responses": 4738, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.09409391673111742, "spearman": 0.0940939167311132, "kendall": 0.0940939167311132}, "p_value": {"pearson": 1.3471396845994733e-12, "spearman": 1.3471396846025803e-12, "kendall": 1.4989542274659266e-12}, "kappa_score": 0.05142428626239515, "total_responses": 5654, "valid_responses": 5602, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.5317234620743982, "spearman": 0.5317234620743703, "kendall": 0.5317234620743703}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.5286945508100147, "total_responses": 5654, "valid_responses": 5618, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.1261272539547136, "spearman": 0.12612725395470903, "kendall": 0.12612725395470906}, "p_value": {"pearson": 1.7344806272393256e-21, "spearman": 1.7344806272451903e-21, "kendall": 2.4693678365204014e-21}, "kappa_score": 0.038245510020886275, "total_responses": 5654, "valid_responses": 5590, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.6920010028357961, "spearman": 0.6920010028357911, "kendall": 0.6920010028357911}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.6919976874080904, "total_responses": 5654, "valid_responses": 5583, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.3141227704094022, "spearman": 0.3141227704094091, "kendall": 0.3141227704094091}, "p_value": {"pearson": 1.0604093537535686e-129, "spearman": 1.0604093537393373e-129, "kendall": 2.53154960473904e-123}, "kappa_score": 0.20108979240524116, "total_responses": 5654, "valid_responses": 5579, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.6545290798929333, "spearman": 0.6545290798928866, "kendall": 0.6545290798928866}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.64529539977943, "total_responses": 5654, "valid_responses": 5651, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.37953635718472606, "spearman": 0.3795363571847244, "kendall": 0.37953635718472445}, "p_value": {"pearson": 3.5348170367915377e-193, "spearman": 3.534817036806945e-193, "kendall": 4.192562537490454e-179}, "kappa_score": 0.2564949271997795, "total_responses": 5654, "valid_responses": 5654, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.4495941478920249, "spearman": 0.4495941478919826, "kendall": 0.44959414789198254}, "p_value": {"pearson": 1.6802672120622152e-279, "spearman": 1.6802672122881941e-279, "kendall": 1.7590240562379818e-250}, "kappa_score": 0.4488599639630614, "total_responses": 5654, "valid_responses": 5201, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.3625804828431093, "spearman": 0.36258048284309696, "kendall": 0.36258048284309696}, "p_value": {"pearson": 3.0160852222141865e-175, "spearman": 3.016085222301962e-175, "kendall": 1.2263782475333452e-163}, "kappa_score": 0.27566932259953847, "total_responses": 5654, "valid_responses": 5649, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "gpt-4o (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.7395797744023942, "spearman": 0.7395797744024001, "kendall": 0.7395797744024}, "p_value": {"pearson": 0.0, "spearman": 0.0, "kendall": 0.0}, "kappa_score": 0.7387577364876226, "total_responses": 5654, "valid_responses": 5647, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.36192074500922666, "spearman": 0.36192074500921584, "kendall": 0.36192074500921584}, "p_value": {"pearson": 1.4304585443434947e-174, "spearman": 1.4304585443801599e-174, "kendall": 4.744066053119717e-163}, "kappa_score": 0.23500887531349568, "total_responses": 5654, "valid_responses": 5654, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"toxicity": {"corr_coeff": {"pearson": 0.4694817621131784, "spearman": 0.46948176211314946, "kendall": 0.46948176211314946}, "p_value": {"pearson": 5.795087202365482e-308, "spearman": 5.79508720293617e-308, "kendall": 6.157035274055427e-273}, "kappa_score": 0.4476953300301513, "total_responses": 5654, "valid_responses": 4971, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}, "jailbreaking": {"corr_coeff": {"pearson": 0.18829031205729402, "spearman": 0.18829031205729443, "kendall": 0.18829031205729443}, "p_value": {"pearson": 2.795968575696913e-46, "spearman": 2.7959685756958978e-46, "kendall": 1.6935400498088967e-45}, "kappa_score": 0.08375751657157382, "total_responses": 5654, "valid_responses": 4796, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Toxicity \\ Safety"}}}