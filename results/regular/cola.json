{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.45769519043583146, "spearman": 0.4576951904358315, "kendall": 0.45769519043583157}, "p_value": {"pearson": 3.919437998192899e-55, "spearman": 3.9194379981928496e-55, "kendall": 2.1421826530625945e-49}, "kappa_score": 0.41999245168806876, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.5023246323897274, "spearman": 0.5023246323897272, "kendall": 0.5023246323897272}, "p_value": {"pearson": 9.055686948890052e-68, "spearman": 9.055686948892195e-68, "kendall": 3.9474605615597803e-59}, "kappa_score": 0.464060251930587, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.43754267028156413, "spearman": 0.43754267028156396, "kendall": 0.43754267028156396}, "p_value": {"pearson": 5.170983221040792e-50, "spearman": 5.170983221041778e-50, "kendall": 2.7063724231989658e-45}, "kappa_score": 0.4282446332374976, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.427567060055693, "spearman": 0.42756706005569295, "kendall": 0.427567060055693}, "p_value": {"pearson": 1.3286641380428592e-47, "spearman": 1.3286641380429107e-47, "kendall": 2.4830805485057537e-43}, "kappa_score": 0.4233610172729897, "total_responses": 1043, "valid_responses": 1027, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.4614640821108973, "spearman": 0.46146408211089607, "kendall": 0.461464082110896}, "p_value": {"pearson": 3.9528917132039964e-56, "spearman": 3.9528917132073597e-56, "kendall": 3.4953311703877482e-50}, "kappa_score": 0.45492772033639395, "total_responses": 1043, "valid_responses": 885, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.049022979618969564, "spearman": 0.049022979618970085, "kendall": 0.049022979618970085}, "p_value": {"pearson": 0.11358679065341669, "spearman": 0.11358679065340954, "kendall": 0.11354391316300363}, "kappa_score": 0.006762593015832152, "total_responses": 1043, "valid_responses": 1042, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.5454833528929087, "spearman": 0.5454833528929038, "kendall": 0.545483352892904}, "p_value": {"pearson": 6.576636526766338e-82, "spearman": 6.576636526791646e-82, "kendall": 2.1305427437293342e-69}, "kappa_score": 0.5191970967699099, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "gpt-4o (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.4231189518534575, "spearman": 0.42311895185345405, "kendall": 0.42311895185345405}, "p_value": {"pearson": 1.4866514197339686e-46, "spearman": 1.4866514197366277e-46, "kendall": 1.80164304184509e-42}, "kappa_score": 0.3405346404837255, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.5124961029736279, "spearman": 0.5124961029736264, "kendall": 0.5124961029736264}, "p_value": {"pearson": 6.393224796626198e-71, "spearman": 6.39322479663314e-71, "kendall": 1.7868721139413494e-61}, "kappa_score": 0.4462224178342259, "total_responses": 1043, "valid_responses": 1040, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.5523177587754771, "spearman": 0.5523177587754702, "kendall": 0.5523177587754701}, "p_value": {"pearson": 2.4277809566970186e-84, "spearman": 2.427780956710955e-84, "kendall": 4.221562910238072e-71}, "kappa_score": 0.5513526670401635, "total_responses": 1043, "valid_responses": 1025, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.5779528976988542, "spearman": 0.5779528976988538, "kendall": 0.5779528976988537}, "p_value": {"pearson": 5.478950630306071e-94, "spearman": 5.478950630307596e-94, "kendall": 1.121678028545913e-77}, "kappa_score": 0.5425669105886788, "total_responses": 1043, "valid_responses": 1043, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"grammaticality": {"corr_coeff": {"pearson": 0.23504520267842532, "spearman": 0.2350452026784226, "kendall": 0.2350452026784226}, "p_value": {"pearson": 1.4738497501589187e-14, "spearman": 1.4738497501599492e-14, "kendall": 3.267285228434348e-14}, "kappa_score": 0.12262148462073774, "total_responses": 1043, "valid_responses": 1038, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Acceptability"}}}