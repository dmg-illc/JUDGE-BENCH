{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.24059634846077382, "spearman": 0.2826374603924494, "kendall": 0.2279347380487319}, "p_value": {"pearson": 0.015898062633121803, "spearman": 0.004383265449373921, "kendall": 0.004212030066011082}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.42394869044952255, "spearman": 0.4505846519092917, "kendall": 0.3667841520868234}, "p_value": {"pearson": 1.1059722136764127e-05, "spearman": 2.549972182913724e-06, "kendall": 3.7494147777081252e-06}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.5333872260798976, "spearman": 0.5240821782003544, "kendall": 0.42512783721713493}, "p_value": {"pearson": 1.1057863500522101e-08, "spearman": 2.1952735163875712e-08, "kendall": 8.207525401191053e-08}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.2855688231764707, "spearman": 0.2967603113451265, "kendall": 0.24166133378617982}, "p_value": {"pearson": 0.003976588007642484, "spearman": 0.0027163307258574665, "kendall": 0.002494497561989875}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 77, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.12296769650243143, "spearman": 0.1284606575244697, "kendall": 0.10731019208540299}, "p_value": {"pearson": 0.22290162116856702, "spearman": 0.20275536718720935, "kendall": 0.19898494483550921}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 95, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.4632272196015049, "spearman": 0.5253916722249533, "kendall": 0.4238828463822977}, "p_value": {"pearson": 1.2155158199502093e-06, "spearman": 1.995801015165169e-08, "kendall": 4.24786896967137e-08}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 93, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.49658326169088784, "spearman": 0.48798773066218554, "kendall": 0.3946527917765272}, "p_value": {"pearson": 1.4832926681485172e-07, "spearman": 2.605979440035481e-07, "kendall": 4.22692628160292e-07}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.6036473047727718, "spearman": 0.5900751012389175, "kendall": 0.4710433994507182}, "p_value": {"pearson": 2.9751043283148624e-11, "spearman": 1.04351553522608e-10, "kendall": 1.3653651168260718e-09}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gpt-4o (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.7090161407521609, "spearman": 0.6551622337452909, "kendall": 0.5258555091409866}, "p_value": {"pearson": 1.529340302207796e-16, "spearman": 1.4111924438992228e-13, "kendall": 1.8267054707324947e-11}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.45999114169074795, "spearman": 0.5580325993542875, "kendall": 0.43428333506289435}, "p_value": {"pearson": 1.473514148850341e-06, "spearman": 1.6210603833596942e-09, "kendall": 1.5702883572060616e-08}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 99, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.6851812148058457, "spearman": 0.6284483869866779, "kendall": 0.51430607054368}, "p_value": {"pearson": 3.760882678274499e-15, "spearman": 2.5609022177722652e-12, "kendall": 2.2672198605033853e-10}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.32709904806495027, "spearman": 0.36459375392060456, "kendall": 0.30213540738437095}, "p_value": {"pearson": 0.0008945841435877097, "spearman": 0.00019199581057422098, "kendall": 0.0002631298852709692}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}}