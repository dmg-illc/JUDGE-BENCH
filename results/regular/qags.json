{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.5966082880318917, "spearman": 0.5966082880318918, "kendall": 0.5966082880318918}, "p_value": {"pearson": 6.028784215685352e-93, "spearman": 6.028784215685262e-93, "kendall": 1.132291083749974e-75}, "kappa_score": 0.5811240605301162, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.6969680379623119, "spearman": 0.696968037962312, "kendall": 0.696968037962312}, "p_value": {"pearson": 1.685079422761599e-139, "spearman": 1.685079422761712e-139, "kendall": 1.4100063867920313e-102}, "kappa_score": 0.6968019445398899, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.4933153040808814, "spearman": 0.4933153040808813, "kendall": 0.49331530408088137}, "p_value": {"pearson": 1.3491772889504344e-59, "spearman": 1.3491772889504588e-59, "kendall": 2.565958585731957e-52}, "kappa_score": 0.433836286520756, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.12726966407814877, "spearman": 0.12726966407814877, "kendall": 0.12726966407814883}, "p_value": {"pearson": 8.155702206181579e-05, "spearman": 8.155702206181123e-05, "kendall": 8.606865827690357e-05}, "kappa_score": 0.1125949587161642, "total_responses": 953, "valid_responses": 740, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.02129255898554531, "spearman": 0.02129255898554536, "kendall": 0.021292558985545364}, "p_value": {"pearson": 0.5114845213344005, "spearman": 0.5114845213343966, "kendall": 0.5111994333483417}, "kappa_score": 0.015122001908198679, "total_responses": 953, "valid_responses": 692, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.3656904957299366, "spearman": 0.3656904957299374, "kendall": 0.36569049572993745}, "p_value": {"pearson": 1.5884019076692069e-31, "spearman": 1.5884019076685075e-31, "kendall": 1.588666739389157e-29}, "kappa_score": 0.3265965525430786, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.6418776155956281, "spearman": 0.6418776155956282, "kendall": 0.6418776155956283}, "p_value": {"pearson": 8.732528806373552e-112, "spearman": 8.73252880637299e-112, "kendall": 2.7057567490794418e-87}, "kappa_score": 0.6405613048892012, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.652708130448279, "spearman": 0.6527081304482758, "kendall": 0.6527081304482758}, "p_value": {"pearson": 8.911905501254289e-117, "spearman": 8.911905501284435e-117, "kendall": 3.361801623517165e-90}, "kappa_score": 0.6466163422063302, "total_responses": 953, "valid_responses": 928, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "gpt-4o (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.7226986322816906, "spearman": 0.7226986322816897, "kendall": 0.7226986322816897}, "p_value": {"pearson": 9.87575946476747e-155, "spearman": 9.87575946478099e-155, "kendall": 3.8206904029417285e-110}, "kappa_score": 0.7222781768450114, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.6845758605075761, "spearman": 0.6845758605075736, "kendall": 0.6845758605075736}, "p_value": {"pearson": 9.944985247181988e-133, "spearman": 9.944985247213085e-133, "kendall": 4.9678213658789246e-99}, "kappa_score": 0.6815089058657045, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.6664493639916144, "spearman": 0.6664493639916134, "kendall": 0.6664493639916135}, "p_value": {"pearson": 2.060521256224052e-123, "spearman": 2.0605212562261128e-123, "kendall": 5.892624912636534e-94}, "kappa_score": 0.6625893978772676, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"Factual Consistency": {"corr_coeff": {"pearson": 0.2511985490372143, "spearman": 0.2511985490372155, "kendall": 0.2511985490372155}, "p_value": {"pearson": 3.5170241483935413e-15, "spearman": 3.5170241483924787e-15, "kendall": 9.145580439990903e-15}, "kappa_score": 0.13398423466260911, "total_responses": 953, "valid_responses": 953, "krippendorff_alpha": 0.4878830126174004, "type": "categorical", "expert": "false", "task": "Summarisation"}}}