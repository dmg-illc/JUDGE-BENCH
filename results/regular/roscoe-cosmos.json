{"Meta-Llama-3.1-8B-Instruct (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.5831319386931707, "spearman": 0.6243019222714528, "kendall": 0.5487881254782515}, "p_value": {"pearson": 3.731464128332566e-19, "spearman": 1.8242600158857694e-22, "kendall": 2.628420569391976e-18}, "kappa_score": 0.014545336745602544, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.2665084421246452, "spearman": 0.25937523737914064, "kendall": 0.23809958197178033}, "p_value": {"pearson": 0.0001658820350811378, "spearman": 0.00025068825932105833, "kendall": 0.00029640553891239867}, "kappa_score": 0.01529693175442226, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": NaN, "spearman": NaN, "kendall": NaN}, "p_value": {"pearson": NaN, "spearman": NaN, "kendall": NaN}, "kappa_score": 0.0, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.18814174662064084, "spearman": 0.18814174662064087, "kendall": 0.18814174662064084}, "p_value": {"pearson": 0.00844030854500073, "spearman": 0.008440308545000661, "kendall": 0.008779802084499315}, "kappa_score": 0.15692554043234574, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Meta-Llama-3.1-70B-Instruct (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6868299817660153, "spearman": 0.6779047200090803, "kendall": 0.5941754641037126}, "p_value": {"pearson": 1.4943719363011136e-28, "spearman": 1.365789548717681e-27, "kendall": 3.5809216091703975e-23}, "kappa_score": 0.2667122478789198, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.4048928859268577, "spearman": 0.4242235621368703, "kendall": 0.36751507547360474}, "p_value": {"pearson": 4.330684840498802e-09, "spearman": 6.399965128188436e-10, "kendall": 2.811096265342568e-09}, "kappa_score": 0.16409912926992642, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.32653965580946287, "spearman": 0.32653965580946287, "kendall": 0.32653965580946287}, "p_value": {"pearson": 3.1788933053485375e-06, "spearman": 3.1788933053486078e-06, "kendall": 5.411412908515016e-06}, "kappa_score": 0.2592277598230991, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.29966683984600884, "spearman": 0.29966683984600884, "kendall": 0.29966683984600884}, "p_value": {"pearson": 2.0817768639481925e-05, "spearman": 2.0817768639482114e-05, "kendall": 2.9946065686362753e-05}, "kappa_score": 0.23232172223052783, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mistral-7B-Instruct-v0.3 (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.7238442949663206, "spearman": 0.7143529823470084, "kendall": 0.6188262609179983}, "p_value": {"pearson": 6.171294830094308e-33, "spearman": 9.567830376167603e-32, "kendall": 2.2801945747699792e-23}, "kappa_score": 0.29449576732586336, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.43167819018939446, "spearman": 0.42678233093477874, "kendall": 0.36844888750741905}, "p_value": {"pearson": 2.9623443380522754e-10, "spearman": 4.923394324311621e-10, "kendall": 3.4424163200924332e-09}, "kappa_score": 0.23630260160652194, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.28509843685981395, "spearman": 0.28509843685981395, "kendall": 0.2850984368598139}, "p_value": {"pearson": 5.350873948247382e-05, "spearman": 5.350873948247344e-05, "kendall": 7.158307985384165e-05}, "kappa_score": 0.2699041955731748, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.42580373261422466, "spearman": 0.4258037326142247, "kendall": 0.42580373261422466}, "p_value": {"pearson": 5.444312960814803e-10, "spearman": 5.444312960814699e-10, "kendall": 3.015362097313445e-09}, "kappa_score": 0.3191736979924352, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "OLMo-7B-0724-Instruct-hf (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6286219031851341, "spearman": 0.6543253913749633, "kendall": 0.5612329402052125}, "p_value": {"pearson": 7.669940675114561e-23, "spearman": 3.297699974345737e-25, "kendall": 1.615616820183392e-20}, "kappa_score": 0.07967795669072741, "total_responses": 195, "valid_responses": 174, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.3126072533099367, "spearman": 0.3162836352763602, "kendall": 0.2754245599398932}, "p_value": {"pearson": 8.616011911846672e-06, "spearman": 6.654787108900765e-06, "kendall": 7.85523608768294e-06}, "kappa_score": 0.11663083538083541, "total_responses": 195, "valid_responses": 173, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.18050644804631216, "spearman": -0.18050644804631216, "kendall": -0.1805064480463121}, "p_value": {"pearson": 0.011563018053063737, "spearman": 0.011563018053063617, "kendall": 0.011931497094196749}, "kappa_score": -0.17680639373674767, "total_responses": 195, "valid_responses": 173, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.2089964207519266, "spearman": -0.20899642075192662, "kendall": -0.2089964207519266}, "p_value": {"pearson": 0.0033660984653762723, "spearman": 0.0033660984653762797, "kendall": 0.0036029327780747237}, "kappa_score": -0.18382978723404242, "total_responses": 195, "valid_responses": 121, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.1074888511093028, "spearman": 0.10225670431538007, "kendall": 0.09126146405908284}, "p_value": {"pearson": 0.13473535402055728, "spearman": 0.15488459091846463, "kendall": 0.14990852189294643}, "kappa_score": 0.03513112320633349, "total_responses": 195, "valid_responses": 188, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.1641975896718836, "spearman": 0.15397551287409175, "kendall": 0.13824555451278445}, "p_value": {"pearson": 0.021805244573606677, "spearman": 0.0316217004530082, "kendall": 0.032488246931217095}, "kappa_score": 0.04128657741973696, "total_responses": 195, "valid_responses": 192, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.043901992749071625, "spearman": 0.04390199274907167, "kendall": 0.04390199274907167}, "p_value": {"pearson": 0.5422524519953514, "spearman": 0.5422524519953502, "kendall": 0.5408792068055606}, "kappa_score": 0.02842907385697535, "total_responses": 195, "valid_responses": 158, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.033013846793582824, "spearman": 0.033013846793582824, "kendall": 0.03301384679358282}, "p_value": {"pearson": 0.6468272624680358, "spearman": 0.6468272624680355, "kendall": 0.645638480072384}, "kappa_score": 0.027379815345431435, "total_responses": 195, "valid_responses": 33, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.5905491179227125, "spearman": 0.6023429771240759, "kendall": 0.5209861510681599}, "p_value": {"pearson": 1.0234023827908447e-19, "spearman": 1.2201589402720042e-20, "kendall": 1.2741436180861277e-17}, "kappa_score": 0.20913533459658917, "total_responses": 195, "valid_responses": 194, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.3712709919322456, "spearman": 0.36630090948485894, "kendall": 0.31497476762072435}, "p_value": {"pearson": 9.121477702792243e-08, "spearman": 1.3910763064334178e-07, "kendall": 5.363783756459339e-07}, "kappa_score": 0.05122555769760406, "total_responses": 195, "valid_responses": 191, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.0068251435956000195, "spearman": 0.006825143595600033, "kendall": 0.006825143595600034}, "p_value": {"pearson": 0.9245560670092506, "spearman": 0.9245560670092485, "kendall": 0.9242645880524719}, "kappa_score": 0.002295223066991814, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.05333536519547213, "spearman": -0.05333536519547222, "kendall": -0.05333536519547223}, "p_value": {"pearson": 0.4589804596535495, "spearman": 0.45898045965354883, "kendall": 0.4575569538176032}, "kappa_score": -0.0199714693295292, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6909849491460673, "spearman": 0.6865109673417613, "kendall": 0.5979700953838191}, "p_value": {"pearson": 5.190932737708795e-29, "spearman": 1.6195684861358152e-28, "kendall": 2.1803115467526643e-23}, "kappa_score": 0.2651875330163761, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.4200524546559989, "spearman": 0.446188888954243, "kendall": 0.39766751113331666}, "p_value": {"pearson": 9.768719483130034e-10, "spearman": 6.2611194321225e-11, "kendall": 4.019545197020865e-10}, "kappa_score": 0.06671336018210472, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.07999288983689688, "spearman": 0.07999288983689659, "kendall": 0.07999288983689659}, "p_value": {"pearson": 0.26629271456460224, "spearman": 0.266292714564603, "kendall": 0.26520539259150755}, "kappa_score": 0.01271635464500176, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.29414311068789656, "spearman": 0.2941431106878968, "kendall": 0.2941431106878967}, "p_value": {"pearson": 2.9957884517870902e-05, "spearman": 2.9957884517870773e-05, "kendall": 4.1864834079191806e-05}, "kappa_score": 0.25872340425531914, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6850211038493614, "spearman": 0.699109070141407, "kendall": 0.5987301149351115}, "p_value": {"pearson": 2.3549805423815507e-28, "spearman": 6.231370658234482e-30, "kendall": 3.2403737292744787e-24}, "kappa_score": 0.16002314517165994, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.39885826591000584, "spearman": 0.4339287146605083, "kendall": 0.3710359688535261}, "p_value": {"pearson": 7.676104624933552e-09, "spearman": 2.338989721413337e-10, "kendall": 8.284916910540362e-10}, "kappa_score": 0.18831731936591622, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.26588026605804627, "spearman": 0.26588026605804627, "kendall": 0.26588026605804627}, "p_value": {"pearson": 0.00017210551077579322, "spearman": 0.0001721055107757953, "kendall": 0.00021282674851752932}, "kappa_score": 0.2637540453074434, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.06604883758217173, "spearman": 0.06604883758217178, "kendall": 0.06604883758217178}, "p_value": {"pearson": 0.3589361069526465, "spearman": 0.35893610695264555, "kendall": 0.3575968738004821}, "kappa_score": 0.06514382402707275, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-4o (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6558310157399654, "spearman": 0.694114002139439, "kendall": 0.606058788592328}, "p_value": {"pearson": 2.357334435053744e-25, "spearman": 2.3134999781338993e-29, "kendall": 9.758155356895183e-24}, "kappa_score": 0.21118214324412177, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.39495871334383525, "spearman": 0.4414254181853633, "kendall": 0.377065033815778}, "p_value": {"pearson": 1.104469096570072e-08, "spearman": 1.0513691356621829e-10, "kendall": 7.076246227131719e-10}, "kappa_score": 0.1511439562664506, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.189978705814295, "spearman": 0.18997870581429532, "kendall": 0.18997870581429527}, "p_value": {"pearson": 0.007811225972010439, "spearman": 0.00781122597201027, "kendall": 0.008142642054837095}, "kappa_score": 0.10742221449678435, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.30217916726754634, "spearman": 0.30217916726754646, "kendall": 0.30217916726754646}, "p_value": {"pearson": 1.759799403948379e-05, "spearman": 1.7597994039483608e-05, "kendall": 2.5665238614382955e-05}, "kappa_score": 0.21038100496962997, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6839581085498314, "spearman": 0.6803256796839753, "kendall": 0.591124413518487}, "p_value": {"pearson": 3.0718421896030794e-28, "spearman": 7.552686156897367e-28, "kendall": 7.616947213694982e-22}, "kappa_score": 0.17820953797527195, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.3822004242163266, "spearman": 0.3861867797008531, "kendall": 0.33533755732046544}, "p_value": {"pearson": 3.515875882234236e-08, "spearman": 2.461506139937692e-08, "kendall": 7.493087221792019e-08}, "kappa_score": 0.1737600726364772, "total_responses": 195, "valid_responses": 194, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": 0.26165034241679463, "spearman": 0.26165034241679413, "kendall": 0.2616503424167941}, "p_value": {"pearson": 0.00022003023329856796, "spearman": 0.0002200302332985748, "kendall": 0.0002680501184456294}, "kappa_score": 0.22057084951866235, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.1569700739803705, "spearman": 0.1569700739803706, "kendall": 0.1569700739803706}, "p_value": {"pearson": 0.028417180312496758, "spearman": 0.0284171803124967, "kendall": 0.02879072963440624}, "kappa_score": 0.15375077495350287, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.6272318743517984, "spearman": 0.6269212615303179, "kendall": 0.5281239778308284}, "p_value": {"pearson": 1.0151121387428979e-22, "spearman": 1.0805130996645009e-22, "kendall": 2.826991035986406e-19}, "kappa_score": 0.23132643778848427, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.39774719622115207, "spearman": 0.3981731514616081, "kendall": 0.3479443098731373}, "p_value": {"pearson": 8.518645742115803e-09, "spearman": 8.185596951122508e-09, "kendall": 1.0954015494551879e-08}, "kappa_score": 0.2312465778426721, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.06227943530985016, "spearman": -0.0622794353098503, "kendall": -0.06227943530985031}, "p_value": {"pearson": 0.38707524589852654, "spearman": 0.38707524589852516, "kendall": 0.38569431501344553}, "kappa_score": -0.025596072931276304, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": 0.28482613755117714, "spearman": 0.2848261375511779, "kendall": 0.28482613755117786}, "p_value": {"pearson": 5.443482921428551e-05, "spearman": 5.4434829214282316e-05, "kendall": 7.273140953177699e-05}, "kappa_score": 0.20901932712956328, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"Overall Quality": {"corr_coeff": {"pearson": 0.4985507412252552, "spearman": 0.5038358263037803, "kendall": 0.41751336770987557}, "p_value": {"pearson": 1.1983856555090276e-13, "spearman": 5.99354254768375e-14, "kendall": 1.798225065354865e-12}, "kappa_score": 0.1419278721979529, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Coherency": {"corr_coeff": {"pearson": 0.17280857369730424, "spearman": 0.14738726449208273, "kendall": 0.1334044430936378}, "p_value": {"pearson": 0.01569942892110914, "spearman": 0.03976693242640714, "kendall": 0.038375178960669734}, "kappa_score": 0.05074914009211218, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "graded", "expert": "true", "task": "Reasoning"}, "Missing Steps": {"corr_coeff": {"pearson": -0.04618877564378561, "spearman": -0.046188775643785596, "kendall": -0.046188775643785596}, "p_value": {"pearson": 0.521401406754885, "spearman": 0.5214014067548843, "kendall": 0.5200066197828697}, "kappa_score": -0.03083700440528636, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}, "Contradiction": {"corr_coeff": {"pearson": -0.05333536519547202, "spearman": -0.05333536519547222, "kendall": -0.05333536519547223}, "p_value": {"pearson": 0.4589804596535502, "spearman": 0.45898045965354883, "kendall": 0.4575569538176032}, "kappa_score": -0.0199714693295292, "total_responses": 195, "valid_responses": 195, "krippendorff_alpha": NaN, "type": "categorical", "expert": "true", "task": "Reasoning"}}}